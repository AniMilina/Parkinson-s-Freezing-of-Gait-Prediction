{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![ML](https://raw.githubusercontent.com/AniMilina/Parkinson-s-Freezing-of-Gait-Prediction/main/ML.jpg)\n","metadata":{}},{"cell_type":"markdown","source":"To detect episodes of freezing of gait (FOG) and classify their types along the timeline, we have the following dataset available:\n\n* Unique patient identifier (stored in the 'Id' column)  \n* Patient number (stored in the 'Subject' column)  \n* Visit number (stored in the 'Visit' column)  \n* Medications administered (stored in the 'Medication' column)  \n* Time of FOG measurement (stored in the 'Time' column)  \n* Start time of FOG event (recorded in the 'Init' column)  \n* End time of FOG event (recorded in the 'Completion' column)  \n* Vertical axis acceleration (captured in the 'AccV' column)  \n* Mediolateral axis acceleration (captured in the 'AccML' column)  \n* Anteroposterior axis acceleration (captured in the 'AccAP' column)  \n* Initial uncertainty at the start of the event (stored in the 'StartHesitation' column)  \n* Uncertainty during turning (stored in the 'Turn' column)  \n* Movement delays (captured in the 'Walking' column)  \n\nOur goal is to develop a model capable of predicting episodes of freezing of gait and their corresponding types using this dataset. To evaluate the model's performance, we will utilize the average sum of AP (mean Average Precision) across all three event classes.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nimport joblib\n\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import average_precision_score,accuracy_score, confusion_matrix, roc_auc_score, f1_score","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-05-20T06:54:48.747771Z","iopub.execute_input":"2023-05-20T06:54:48.748075Z","iopub.status.idle":"2023-05-20T06:54:50.579119Z","shell.execute_reply.started":"2023-05-20T06:54:48.748048Z","shell.execute_reply":"2023-05-20T06:54:50.578077Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Settings","metadata":{}},{"cell_type":"code","source":"# # Set formatting option\n\npd.options.display.float_format = '{:.2f}'.format","metadata":{"execution":{"iopub.status.busy":"2023-05-20T06:54:50.585593Z","iopub.execute_input":"2023-05-20T06:54:50.588730Z","iopub.status.idle":"2023-05-20T06:54:50.596362Z","shell.execute_reply.started":"2023-05-20T06:54:50.588693Z","shell.execute_reply":"2023-05-20T06:54:50.594990Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def fill_missing_values(df):\n    \"\"\"\n    Replaces missing values with the median value for each numerical column.\n\n    :param df: pandas DataFrame, the dataset in which missing values need to be replaced\n    :return: pandas DataFrame, the dataset with replaced missing values\n    \"\"\"\n    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns  # Selecting all numerical columns\n    for col in numeric_cols:\n        median = df[col].median()  # Finding the Median Value of a Column\n        df[col].fillna(median, inplace=True)  # Replacing Missing Values with the Median Value\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-05-20T06:54:50.605832Z","iopub.execute_input":"2023-05-20T06:54:50.607212Z","iopub.status.idle":"2023-05-20T06:54:50.617631Z","shell.execute_reply.started":"2023-05-20T06:54:50.607180Z","shell.execute_reply":"2023-05-20T06:54:50.616875Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Function to Get Data Information\n\ndef explore_dataframe(df):\n    print(\"Shape of dataframe:\", df.shape)\n    display(df.head())\n    print(\"Info of dataframe:\\n\")\n    df.info()\n    print(\"Summary statistics of dataframe:\\n\", df.describe())\n    print(\"Missing values in dataframe:\\n\", df.isnull().sum())\n    print(\"Duplicate rows in dataframe:\", df.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2023-05-20T06:54:50.622547Z","iopub.execute_input":"2023-05-20T06:54:50.624072Z","iopub.status.idle":"2023-05-20T06:54:50.635924Z","shell.execute_reply.started":"2023-05-20T06:54:50.624039Z","shell.execute_reply":"2023-05-20T06:54:50.635054Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Checking for Missing Values in Each Column\n\ndef check_missing_values(df):\n    \"\"\"\n    Checks the count of missing values in each column of a DataFrame.\n\n    :param df: pandas.DataFrame, the DataFrame to check for missing values.\n    :return: pandas.DataFrame, the DataFrame with information about missing values.\n    \"\"\"\n    return df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-20T06:54:50.637499Z","iopub.execute_input":"2023-05-20T06:54:50.638274Z","iopub.status.idle":"2023-05-20T06:54:50.647736Z","shell.execute_reply.started":"2023-05-20T06:54:50.638231Z","shell.execute_reply":"2023-05-20T06:54:50.646759Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Reading Data","metadata":{}},{"cell_type":"code","source":"# Reading Data from a CSV File\n\ndata = pd.read_csv('/kaggle/input/eda-parkinson/EDA_Parkinson.csv', low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T06:54:50.649421Z","iopub.execute_input":"2023-05-20T06:54:50.650367Z","iopub.status.idle":"2023-05-20T06:55:37.125867Z","shell.execute_reply.started":"2023-05-20T06:54:50.650329Z","shell.execute_reply":"2023-05-20T06:55:37.124950Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-05-20T06:55:37.127110Z","iopub.execute_input":"2023-05-20T06:55:37.127440Z","iopub.status.idle":"2023-05-20T06:55:37.160132Z","shell.execute_reply.started":"2023-05-20T06:55:37.127409Z","shell.execute_reply":"2023-05-20T06:55:37.159312Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                  Id Subject  Visit Medication    Time    Init  Completion  \\\n0         003f117e14  4dc2f8      3         on    0.00    8.61       14.77   \n1         003f117e14  4dc2f8      3         on    1.00    8.61       14.77   \n2         003f117e14  4dc2f8      3         on    2.00    8.61       14.77   \n3         003f117e14  4dc2f8      3         on    3.00    8.61       14.77   \n4         003f117e14  4dc2f8      3         on    4.00    8.61       14.77   \n...              ...     ...    ...        ...     ...     ...         ...   \n15856286  f9fc61ce85  040587      1         on 4469.00 1172.65     1173.44   \n15856287  f9fc61ce85  040587      1         on 4470.00 1172.65     1173.44   \n15856288  f9fc61ce85  040587      1         on 4471.00 1172.65     1173.44   \n15856289  f9fc61ce85  040587      1         on 4472.00 1172.65     1173.44   \n15856290  f9fc61ce85  040587      1         on 4473.00 1172.65     1173.44   \n\n          AccV  AccML  AccAP  StartHesitation  Turn  Walking  \n0        -9.78   0.11  -0.54             0.00  1.00     0.00  \n1        -9.79   0.09  -0.53             0.00  1.00     0.00  \n2        -9.79   0.07  -0.54             0.00  1.00     0.00  \n3        -9.78   0.12  -0.55             0.00  1.00     0.00  \n4        -9.79   0.12  -0.54             0.00  1.00     0.00  \n...        ...    ...    ...              ...   ...      ...  \n15856286 -9.68   0.55  -0.88             0.00  1.00     0.00  \n15856287 -9.56   0.52  -0.81             0.00  1.00     0.00  \n15856288 -9.61   0.57  -0.79             0.00  1.00     0.00  \n15856289 -9.54   0.49  -0.73             0.00  1.00     0.00  \n15856290 -9.59   0.57  -0.75             0.00  1.00     0.00  \n\n[15856291 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Subject</th>\n      <th>Visit</th>\n      <th>Medication</th>\n      <th>Time</th>\n      <th>Init</th>\n      <th>Completion</th>\n      <th>AccV</th>\n      <th>AccML</th>\n      <th>AccAP</th>\n      <th>StartHesitation</th>\n      <th>Turn</th>\n      <th>Walking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>003f117e14</td>\n      <td>4dc2f8</td>\n      <td>3</td>\n      <td>on</td>\n      <td>0.00</td>\n      <td>8.61</td>\n      <td>14.77</td>\n      <td>-9.78</td>\n      <td>0.11</td>\n      <td>-0.54</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>003f117e14</td>\n      <td>4dc2f8</td>\n      <td>3</td>\n      <td>on</td>\n      <td>1.00</td>\n      <td>8.61</td>\n      <td>14.77</td>\n      <td>-9.79</td>\n      <td>0.09</td>\n      <td>-0.53</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>003f117e14</td>\n      <td>4dc2f8</td>\n      <td>3</td>\n      <td>on</td>\n      <td>2.00</td>\n      <td>8.61</td>\n      <td>14.77</td>\n      <td>-9.79</td>\n      <td>0.07</td>\n      <td>-0.54</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003f117e14</td>\n      <td>4dc2f8</td>\n      <td>3</td>\n      <td>on</td>\n      <td>3.00</td>\n      <td>8.61</td>\n      <td>14.77</td>\n      <td>-9.78</td>\n      <td>0.12</td>\n      <td>-0.55</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>003f117e14</td>\n      <td>4dc2f8</td>\n      <td>3</td>\n      <td>on</td>\n      <td>4.00</td>\n      <td>8.61</td>\n      <td>14.77</td>\n      <td>-9.79</td>\n      <td>0.12</td>\n      <td>-0.54</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15856286</th>\n      <td>f9fc61ce85</td>\n      <td>040587</td>\n      <td>1</td>\n      <td>on</td>\n      <td>4469.00</td>\n      <td>1172.65</td>\n      <td>1173.44</td>\n      <td>-9.68</td>\n      <td>0.55</td>\n      <td>-0.88</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>15856287</th>\n      <td>f9fc61ce85</td>\n      <td>040587</td>\n      <td>1</td>\n      <td>on</td>\n      <td>4470.00</td>\n      <td>1172.65</td>\n      <td>1173.44</td>\n      <td>-9.56</td>\n      <td>0.52</td>\n      <td>-0.81</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>15856288</th>\n      <td>f9fc61ce85</td>\n      <td>040587</td>\n      <td>1</td>\n      <td>on</td>\n      <td>4471.00</td>\n      <td>1172.65</td>\n      <td>1173.44</td>\n      <td>-9.61</td>\n      <td>0.57</td>\n      <td>-0.79</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>15856289</th>\n      <td>f9fc61ce85</td>\n      <td>040587</td>\n      <td>1</td>\n      <td>on</td>\n      <td>4472.00</td>\n      <td>1172.65</td>\n      <td>1173.44</td>\n      <td>-9.54</td>\n      <td>0.49</td>\n      <td>-0.73</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>15856290</th>\n      <td>f9fc61ce85</td>\n      <td>040587</td>\n      <td>1</td>\n      <td>on</td>\n      <td>4473.00</td>\n      <td>1172.65</td>\n      <td>1173.44</td>\n      <td>-9.59</td>\n      <td>0.57</td>\n      <td>-0.75</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n<p>15856291 rows Ã— 13 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Removing unnecessary columns\n\ndata.drop(['Id', 'Subject', 'Visit', 'Medication'], axis=1, inplace=True)\n\n# Splitting into features (X) and target variable (y)\n\nX = data.drop(['StartHesitation', 'Turn', 'Walking'], axis=1)\ny = data[['StartHesitation', 'Turn', 'Walking']]\n\n# Splitting into training and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Creating a pipeline for each column\n\npipelines = {}\nhyperparameters = {\n    'max_depth': 3,\n    'n_estimators': 100,\n    'learning_rate': 0.1\n}\n\n# Processing each column separately\n\nfor column in y.columns:\n    pipeline = make_pipeline(StandardScaler(), xgb.XGBClassifier(**hyperparameters))\n    pipeline.fit(X_train, y_train[column])\n    pipelines[column] = pipeline\n\n# Model evaluation\n\nfor column, pipeline in pipelines.items():\n    train_score = pipeline.score(X_train, y_train[column])\n    test_score = pipeline.score(X_test, y_test[column])\n    print(f\"{column}: Train Score - {train_score}, Test Score - {test_score}\")\n\n# Predictions on the test set\n\npredictions = pd.DataFrame()\n\nfor column, pipeline in pipelines.items():\n    predictions[column] = pipeline.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T06:55:37.161819Z","iopub.execute_input":"2023-05-20T06:55:37.162552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metrics evaluation for each column\n\nfor column in predictions.columns:  \n    y_train_pred = pipeline.predict(X_train)  \n    y_test_pred = pipeline.predict(X_test) \n\n    train_accuracy = accuracy_score(y_train[column], y_train_pred)\n    test_accuracy = accuracy_score(y_test[column], y_test_pred)\n\n    average_precision = average_precision_score(y_test[column], y_test_pred)\n\n    confusion = confusion_matrix(y_test[column], y_test_pred)\n\n    try:\n        roc_auc = roc_auc_score(y_test[column], y_test_pred)\n    except ValueError:\n        roc_auc = \"Not defined\"\n\n    f1 = f1_score(y_test[column], y_test_pred)\n\n    print(f\"Metrics for column '{column}':\")\n    print(f\"Train Accuracy: {train_accuracy}\")\n    print(f\"Test Accuracy: {test_accuracy}\")\n    print(f\"Average Precision: {average_precision}\")\n    print(\"Confusion Matrix:\")\n    print(confusion)\n    print(f\"ROC AUC Score: {roc_auc}\")\n    print(f\"F1 Score: {f1}\")\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the sample_submission.csv file\n\ntest_data = pd.read_csv('/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/sample_submission.csv')\n\n# Creating the submission dataframe\n\nsubmission_df = pd.DataFrame({'Id': test_data['Id'], 'StartHesitation': predictions['StartHesitation'], 'Turn': predictions['Turn'], 'Walking': predictions['Walking']})\n\n# Merging with the sample_submission.csv file\n\nsubmission_df = submission_df.merge(test_data, on='Id', how='inner')\nsubmission_df.drop(['StartHesitation_y', 'Turn_y', 'Walking_y'], axis=1, inplace=True)\nsubmission_df.rename(columns={'StartHesitation_x': 'StartHesitation', 'Turn_x': 'Turn', 'Walking_x': 'Walking'}, inplace=True)\n\n# Saving the submission file as submission.csv\n\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib\n\n# Saving the pipelines\n\njoblib.dump(pipelines, 'pipeline.pkl')\n\n# Loading the pipelines\n\npredictions = joblib.load('pipeline.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion:\n\nOur model for predicting episodes of mobility impairment and their types is based on utilizing various features from the dataset. We incorporated patient information, including their identifiers, numbers, and visits, as well as data on administered medications. Additionally, we utilized information on the timing of FOG measurements, the start and completion times of FOG events, and acceleration data from the vertical, mediolateral, and anteroposterior axes.  \n\nTo train the model, we employed a pipeline consisting of several stages.  \n\n* Firstly, we performed the separation of data into **features (X)** and the **target variable (y)**, where X contains all columns except those related to the target variable, and y contains the `'StartHesitation'`, `'Turn'`, and `'Walking'` columns.  \n\n* Next, we applied the train-test split method, using the train_test_split function with a **test_size of 0.2**.\n\nFor each target variable column, we created a separate pipeline comprising data scaling using StandardScaler and training an XGBClassifier with specific hyperparameters. We chose **XGBClassifier** due to its powerful gradient boosting capabilities, enabling it to handle complex dependencies in the data.\n\nTraining the model for each target variable class was necessary as each class represents a distinct type of mobility impairment episode. Our model can account for the differences and characteristics of each class, resulting in more accurate predictions and classification of FOG episodes.\n\nIn conclusion, we obtained a model capable of predicting episodes of mobility impairment and their types based on patient data and acceleration along different axes. Incorporating various features and training for each target variable class enhances the prediction quality and enables personalized approaches to the treatment and monitoring of Parkinson's disease patients.  \n","metadata":{}}]}